<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Jiwan Chung | Home</title>
  <meta name="description" content="Jiwan Chung">
  <meta name="author" content="Jiwan Chung">
  <meta property="og:title" content="Jiwan Chung" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://jiwanchung.github.io" />
  <meta property="og:site_name" content="Jiwan Chung" />
  <link rel="canonical" href="https://jiwanchung.github.io" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– 
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css> -->
  <link rel="stylesheet" href=/libs/external/fontawesome-free-6.6.0-web/css/all.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img class="u-max-full-width" src='/assets/profile.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Jiwan Chung</h1>
            <p>Ph.D. Candidate, Yonsei University</p>
            <p>jiwan.chung.research@gmail.com</p>
            <p>
              <!-- <span onclick="window.open('')" style="cursor: pointer"> -->
                <!-- <i class="fa-brands fa-bluesky" style="padding-top: 10px"></i> -->
              <!-- </span> -->

              <span onclick="window.open('https://twitter.com/JiwanChung')" style="cursor: pointer">
                <i class="fa-brands fa-twitter fa-lg"></i>
              </span>

              <span onclick="window.open('https://www.linkedin.com/in/jiwan-chung-81231b245')" style="cursor: pointer">
                <i class="fa-brands fa-linkedin fa-lg"></i>
              </span>

              <span onclick="window.open('https://github.com/JiwanChung')" style="cursor: pointer">
                <i class="fa-brands fa-github fa-lg"></i>
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=l4UBOZAAAAAJ&hl=en')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>Bio</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=/index.html#prospective-students>Prospective Students</a></li> -->
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Publications</a></li>
          <!--<li class="navbar-item"><a class="navbar-link" href=/index.html#projects>Projects</a></li>-->
          <li class="navbar-item"><a class="navbar-link" href=/index.html#resume>Vitae</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========== BIO ========== -->
<div class="docs-section" id="bio">
  <h4>Bio</h4>
  <p>
    I'm a Ph.D. candidate at Yonsei University CIP Lab, currently advised by Professor Seon Joo Kim.
  </p>

  <p>
    My research centers on multimodal understanding and generation, with a particular focus on Multimodal Large Language Models (MLLMs).
    I explore how images and text can be more effectively connected in AI systems, spanning both architectural innovations and evaluative methodologies.
  </p>

  <p>
    Here are some keywords about my current research interests:
  </p>

  <ul>
    <li>Architectural improvements to MLLMs (e.g., <a href="">Decoding</a>, <a href="">Fine-tuning</a>)</li>
    <li>Understanding and enhancing multimodal reasoning (e.g., <a href="">Grounded reasoning</a>, <a href="tbu">Fine-grained analysis</a>)</li>
    <li>Applications of multimodal reasoning (e.g., Computer-use agents, Embodied AI)</li>
  </ul>
</div>

<div class="docs-section" id="experience">
  <h4 margin-bottom: 40px;>Research Experience</h4>
  
    <div class="paper" style="margin-bottom: 40px;">
        <div class="row" style="margin: inherit;">
          <div class="three columns">
            <div class="figure">
              
                <img src="/assets/logos/msr.jpeg" width="100%" style="display: block; margin: auto; padding-bottom: 3%; max-width: 200px;">
              
            </div>
          </div>
          <div class="nine columns">
            <p><b>Microsoft Research, AI Frontiers <b style="float:right;"><i>Redmond, U.S.</i></b></b></p>
            <p>Research Scientist Intern <b style="float:right;"><i>Summer 2025</i></b></p>
            
          </div>
      </div>
    </div>
  
    <div class="paper" style="margin-bottom: 40px;">
        <div class="row" style="margin: inherit;">
          <div class="three columns">
            <div class="figure">
              
                <img src="/assets/logos/lgresearch.jpg" width="100%" style="display: block; margin: auto; padding-bottom: 3%; max-width: 200px;">
              
            </div>
          </div>
          <div class="nine columns">
            <p><b>LG AI Research, AML <b style="float:right;"><i>Seoul, South Korea</i></b></b></p>
            <p>Research Scientist Intern <b style="float:right;"><i>Summer 2024</i></b></p>
            
          </div>
      </div>
    </div>
  
    <div class="paper" style="margin-bottom: 40px;">
        <div class="row" style="margin: inherit;">
          <div class="three columns">
            <div class="figure">
              
                <img src="/assets/logos/naver.png" width="100%" style="display: block; margin: auto; padding-bottom: 3%; max-width: 200px;">
              
            </div>
          </div>
          <div class="nine columns">
            <p><b>Naver, Foundational Research <b style="float:right;"><i>Seongnam, South Korea</i></b></b></p>
            <p>Research Scientist Intern <b style="float:right;"><i>Summer 2023</i></b></p>
            
          </div>
      </div>
    </div>
  
    <div class="paper" style="margin-bottom: 40px;">
        <div class="row" style="margin: inherit;">
          <div class="three columns">
            <div class="figure">
              
                <img src="/assets/logos/yonsei.png" width="55%" style="display: block; margin: auto; padding-bottom: 3%; max-width: 200px;">
              
            </div>
          </div>
          <div class="nine columns">
            <p><b>Yonsei University <b style="float:right;"><i>Seoul, South Korea</i></b></b></p>
            <p>Ph.D. Student <b style="float:right;"><i>Mar. 2024 - Present</i></b></p>
            
              <p>Advisor: Seon Joo Kim</p>
            
          </div>
      </div>
    </div>
  
    <div class="paper" style="margin-bottom: 40px;">
        <div class="row" style="margin: inherit;">
          <div class="three columns">
            <div class="figure">
              
                <img src="/assets/logos/snu.png" width="55%" style="display: block; margin: auto; padding-bottom: 3%; max-width: 200px;">
              
            </div>
          </div>
          <div class="nine columns">
            <p><b>Seoul National University <b style="float:right;"><i>Seoul, South Korea</i></b></b></p>
            <p>M.Sc. Student <b style="float:right;"><i>Mar. 2020 - Feb. 2023</i></b></p>
            
              <p>Advisor: Gunhee Kim</p>
            
          </div>
      </div>
    </div>
  
</div>

<!-- ========== PUBLICATIONS ========== -->
<div class="docs-section" id="publications">
  <h4>Publications</h4>

  <p>Most recent publications on <a href="https://scholar.google.com/citations?user=l4UBOZAAAAAJ&hl=en" target="_blank">Google Scholar</a>.<br/>
  <sup>‡</sup> indicates equal contribution.
  </p>

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#papers-selected">Selected</div></li>
    <li><div class="button" data-ref="#papers-all">All</div></li>
  </ul>

  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
                  <p class="title"><b>What MLLMs Learn When They Learn Multimodal Reasoning: Perception, Reasoning, or Integration?</b></p>
          <p><b>Jiwan Chung</b>, Neel Joshi, Pratyusha Sharma, Youngjae Yu, Vibhav Vineet</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="tbu" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning</b></p>
          <p><b>Jiwan Chung‡</b>, Junhyeok Kim‡, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2505.18842" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/jun297/v1" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/kjunh/v1g-sample" target="_blank">Data</a>
            

            
              <a class="button" href="https://huggingface.co/kjunh/v1-7B" target="_blank">Model</a>
            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Teaching Metric Distance to Discrete Autoregressive Language Models</b></p>
          <p><b>Jiwan Chung</b>, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.02379" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?</b></p>
          <p><b>Jiwan Chung</b>, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu</p>
          <p><i>ACL. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2505.24211" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/ACON" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues</b></p>
          <p><b>Jiwan Chung‡</b>, Youngmin Kim‡, Jisoo Kim, sunghyun lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu</p>
          <p><i>ACL. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00958" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>MASS: Overcoming Language Bias in Image-Text Matching</b></p>
          <p><b>Jiwan Chung</b>, Seungwon Lim, Sangkyu Lee, Youngjae Yu</p>
          <p><i>AAAI. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2501.11469" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Towards Visual Text Design Transfer Across Languages</b></p>
          <p><b>Jiwan Chung‡</b>, Yejin Choi‡, Sumin Shim, Giyeong Oh, Youngjae Yu</p>
          <p><i>NeurIPS Datasets and Benchmarks. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.18823v1" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://huggingface.co/datasets/yejinc/MuST-Bench" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding</b></p>
          <p><b>Jiwan Chung‡</b>, Sungjae Lee‡, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu</p>
          <p><i>EMNLP. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.18925" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/VisArgs" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/visargs" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!</b></p>
          <p><b>Jiwan Chung</b>, Seungwon Lim, Jaehyun Jeon, Seungbeen Lee, Youngjae Yu</p>
          <p><i>EMNLP. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.01023" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/VisualPun_UNPIE" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/VisualPun_UNPIE" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>VLIS: Unimodal Language Models Guide Multimodal Language Generation</b></p>
          <p><b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>EMNLP. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.09767" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/vlis" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Fusing pre-trained language models with multimodal prompts through reinforcement learning</b></p>
          <p><b>Jiwan Chung‡</b>, Youngjae Yu‡, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi</p>
          <p><i>CVPR. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/esper" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>ACAV100M: Automatic curation of large-scale datasets for audio-visual video representation learning</b></p>
          <p><b>Jiwan Chung‡</b>, Sangho Lee‡, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, Yale Song</p>
          <p><i>ICCV. 2021.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2021/html/Lee_ACAV100M_Automatic_Curation_of_Large-Scale_Datasets_for_Audio-Visual_Video_Representation_ICCV_2021_paper.html" target="_blank">Paper</a>
            

            
              <a class="button" href="https://acav100m.github.io/" target="_blank">Website</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sangho-vision/acav100m" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
                  <p class="title"><b>Transitional adaptation of pretrained models for visual storytelling</b></p>
          <p><b>Jiwan Chung‡</b>, Youngjae Yu‡, Heeseung Yun, Jongseok Kim, Gunhee Kim</p>
          <p><i>CVPR. 2021.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/tapm" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
    </div>

    <div class="tab-pane" id="papers-all">
      
        <div class="paper">
          <p class="title"><b>What MLLMs Learn When They Learn Multimodal Reasoning: Perception, Reasoning, or Integration?</b></p>
          <p><b>Jiwan Chung</b>, Neel Joshi, Pratyusha Sharma, Youngjae Yu, Vibhav Vineet</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="tbu" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning</b></p>
          <p><b>Jiwan Chung‡</b>, Junhyeok Kim‡, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2505.18842" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/jun297/v1" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/kjunh/v1g-sample" target="_blank">Data</a>
            

            
              <a class="button" href="https://huggingface.co/kjunh/v1-7B" target="_blank">Model</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Teaching Metric Distance to Discrete Autoregressive Language Models</b></p>
          <p><b>Jiwan Chung</b>, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu</p>
          <p><i>arXiv. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.02379" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms</b></p>
          <p>Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, <b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>EMNLP. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.14427" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VAGUE: Visual Contexts Clarify Ambiguous Expressions</b></p>
          <p>Heejeong Nam, Jinwoo Ahn, Keummin Ka, <b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>ICCV. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2411.14137" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?</b></p>
          <p><b>Jiwan Chung</b>, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu</p>
          <p><i>ACL. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2505.24211" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/ACON" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues</b></p>
          <p><b>Jiwan Chung‡</b>, Youngmin Kim‡, Jisoo Kim, sunghyun lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu</p>
          <p><i>ACL. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00958" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</b></p>
          <p>Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, <b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>ICRA. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.01273" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild</b></p>
          <p>Junhyeok Kim, Min Soo Kim, <b>Jiwan Chung</b>, Jungbin Cho, Jisoo Kim, Sungwoong Kim, Gyeongbo Sim, Youngjae Yu</p>
          <p><i>NAACL Findings. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2502.14892" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</b></p>
          <p>Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, <b>Jiwan Chung</b>, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu</p>
          <p><i>NAACL Findings. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.14703" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MASS: Overcoming Language Bias in Image-Text Matching</b></p>
          <p><b>Jiwan Chung</b>, Seungwon Lim, Sangkyu Lee, Youngjae Yu</p>
          <p><i>AAAI. 2025.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2501.11469" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Visual Text Design Transfer Across Languages</b></p>
          <p><b>Jiwan Chung‡</b>, Yejin Choi‡, Sumin Shim, Giyeong Oh, Youngjae Yu</p>
          <p><i>NeurIPS Datasets and Benchmarks. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.18823v1" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://huggingface.co/datasets/yejinc/MuST-Bench" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding</b></p>
          <p><b>Jiwan Chung‡</b>, Sungjae Lee‡, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu</p>
          <p><i>EMNLP. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.18925" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/VisArgs" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/visargs" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!</b></p>
          <p><b>Jiwan Chung</b>, Seungwon Lim, Jaehyun Jeon, Seungbeen Lee, Youngjae Yu</p>
          <p><i>EMNLP. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.01023" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/VisualPun_UNPIE" target="_blank">Code</a>
            

            
              <a class="button" href="https://huggingface.co/datasets/jiwan-chung/VisualPun_UNPIE" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models</b></p>
          <p>Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, <b>Jiwan Chung</b>, Youngjae Yu, Jinyoung Yeo</p>
          <p><i>EMNLP. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.02575" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>HyperCLOVA X Technical Report</b></p>
          <p>HyperCLOVA X Team</p>
          <p><i>arXiv. 2024.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.01954" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Long Story Short: a Summarize-then-Search Method for Long Video Question Answering</b></p>
          <p><b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>BMVC. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2311.01233" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/long-story-short" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VLIS: Unimodal Language Models Guide Multimodal Language Generation</b></p>
          <p><b>Jiwan Chung</b>, Youngjae Yu</p>
          <p><i>EMNLP. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.09767" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/vlis" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Reading books is great, but not if you are driving! Visually grounded reasoning about defeasible commonsense norms</b></p>
          <p>Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, <b>Jiwan Chung</b>, Yejin Son, Yejin Choi, Youngjae Yu</p>
          <p><i>EMNLP. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.10418" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Fusing pre-trained language models with multimodal prompts through reinforcement learning</b></p>
          <p><b>Jiwan Chung‡</b>, Youngjae Yu‡, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi</p>
          <p><i>CVPR. 2023.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/esper" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>ACAV100M: Automatic curation of large-scale datasets for audio-visual video representation learning</b></p>
          <p><b>Jiwan Chung‡</b>, Sangho Lee‡, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, Yale Song</p>
          <p><i>ICCV. 2021.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2021/html/Lee_ACAV100M_Automatic_Curation_of_Large-Scale_Datasets_for_Audio-Visual_Video_Representation_ICCV_2021_paper.html" target="_blank">Paper</a>
            

            
              <a class="button" href="https://acav100m.github.io/" target="_blank">Website</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sangho-vision/acav100m" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Transitional adaptation of pretrained models for visual storytelling</b></p>
          <p><b>Jiwan Chung‡</b>, Youngjae Yu‡, Heeseung Yun, Jongseok Kim, Gunhee Kim</p>
          <p><i>CVPR. 2021.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html" target="_blank">Paper</a>
            

            

            

            

            

            
              <a class="button" href="https://github.com/JiwanChung/tapm" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Character grounding and re-identification in story of videos and text descriptions</b></p>
          <p>Youngjae Yu, Jongseok Kim, Heeseung Yun, <b>Jiwan Chung</b>, Gunhee Kim</p>
          <p><i>ECCV. 2020.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="/assets/papers/2020eccv_cisin.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

          </div>
        </div>
      
    </div>
  </div>
</div>

<!-- ========== RESUME ========== -->
<div class="docs-section" id="resume">
  <h4>Vitæ</h4>

  <p>Full Resume in <a href=/assets/papers/cv.pdf target="_blank">PDF</a>.</p>

  <!-- The Timeline -->
  <ul class="timeline">
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Microsoft Research, AI Frontiers</span>
          <span class="time-wrapper"><span class="time">Summer 2025</span></span>
        </div>
        <div class="desc"><b>Research Scientist Intern</b> <br/> </div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">LG AI Research, AML</span>
          <span class="time-wrapper"><span class="time">Summer 2024</span></span>
        </div>
        <div class="desc"><b>Research Scientist Intern</b> <br/> </div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Naver, Foundational Research</span>
          <span class="time-wrapper"><span class="time">Summer 2023</span></span>
        </div>
        <div class="desc"><b>Research Scientist Intern</b> <br/> </div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-r">
      
        <div class="flag-wrapper">
          <span class="flag">Yonsei University</span>
          <span class="time-wrapper"><span class="time">Mar. 2024 - Present</span></span>
        </div>
        <div class="desc"><b>Ph.D. Student</b> <br/> </div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-r">
      
        <div class="flag-wrapper">
          <span class="flag">Seoul National University</span>
          <span class="time-wrapper"><span class="time">Mar. 2020 - Feb. 2023</span></span>
        </div>
        <div class="desc"><b>M.Sc. Student</b> <br/> </div>
      </div>
    </li>
    
  </ul>
</div>

<!-- <div class="docs-section" id="template"> -->
  <!-- <h4>Website Design</h4> -->
  <!-- Since I made this website, many people have found this Jekyll template useful [ -->
  <!--  -->
  <!-- <a href="https://people.csail.mit.edu/davidam/" target="_blank">1</a>, -->
  <!--  -->
  <!-- <a href="https://thashim.github.io/" target="_blank">2</a>, -->
  <!--  -->
  <!-- <a href="https://pauljwright.github.io/" target="_blank">3</a>, -->
  <!--  -->
  <!-- <a href="https://bayesianbrad.github.io/" target="_blank">4</a>, -->
  <!--  -->
  <!-- <a href="https://ahxt.github.io/" target="_blank">5</a>, -->
  <!--  -->
  <!-- <a href="https://code-terminator.github.io/" target="_blank">6</a>, -->
  <!--  -->
  <!-- <a href="https://dinacmistry.github.io/" target="_blank">7</a>, -->
  <!--  -->
  <!-- <a href="https://stanniszhou.github.io/" target="_blank">8</a>, -->
  <!--  -->
  <!-- <a href="http://binghongchen.net/" target="_blank">9</a>, -->
  <!--  -->
  <!-- <a href="https://jirvin16.github.io/" target="_blank">10</a>, -->
  <!--  -->
  <!-- <a href="https://mengliu1998.github.io/" target="_blank">11</a>, -->
  <!--  -->
  <!-- <a href="https://amir-rahimi.github.io/" target="_blank">12</a>, -->
  <!--  -->
  <!-- <a href="https://taoyds.github.io/" target="_blank">13</a>, -->
  <!--  -->
  <!-- <a href="https://zhangjingtun.com/" target="_blank">14</a>, -->
  <!--  -->
  <!-- <a href="https://www.saraiht.com/" target="_blank">15</a>, -->
  <!--  -->
  <!-- <a href="https://tomyan555.github.io/" target="_blank">16</a>, -->
  <!--  -->
  <!-- <a href="https://abhishekgrewal.github.io/" target="_blank">17</a>, -->
  <!--  -->
  <!-- <a href="https://xuezhemax.github.io/" target="_blank">18</a>, -->
  <!--  -->
  <!-- <a href="https://mmeendez8.github.io/" target="_blank">19</a>, -->
  <!--  -->
  <!-- <a href="https://pkassraie.github.io/" target="_blank">20</a>, -->
  <!--  -->
  <!-- <a href="https://ninasakhnini.dev/" target="_blank">21</a>, -->
  <!--  -->
  <!-- <a href="http://www.yapengtian.com/" target="_blank">22</a>, -->
  <!--  -->
  <!-- <a href="https://weijian-li.github.io/" target="_blank">23</a>, -->
  <!--  -->
  <!-- <a href="https://www.seas.upenn.edu/~elenter/" target="_blank">24</a>, -->
  <!--  -->
  <!-- <a href="https://cm-bf.github.io/" target="_blank">25</a>, -->
  <!--  -->
  <!-- <a href="https://stuartburrell.github.io/" target="_blank">26</a>, -->
  <!--  -->
  <!-- <a href="https://willwang.netlify.app/" target="_blank">27</a>, -->
  <!--  -->
  <!-- <a href="https://blandocs.github.io/" target="_blank">28</a>, -->
  <!--  -->
  <!-- <a href="https://ninyam.github.io/es/" target="_blank">29</a>, -->
  <!--  -->
  <!-- <a href="https://andrewoabel.github.io/" target="_blank">30</a>, -->
  <!--  -->
  <!-- <a href="https://littlewhitesea.github.io/" target="_blank">31</a>, -->
  <!--  -->
  <!-- <a href="https://jncsw.github.io/" target="_blank">32</a>, -->
  <!--  -->
  <!-- <a href="https://leenacvankadara.com/" target="_blank">33</a>, -->
  <!--  -->
  <!-- <a href="https://mmoorr.github.io/www_personal/" target="_blank">34</a>, -->
  <!--  -->
  <!-- <a href="https://citychan.github.io/" target="_blank">35</a>, -->
  <!--  -->
  <!-- <a href="https://www.zitaoliu.com/" target="_blank">36</a>, -->
  <!--  -->
  <!-- <a href="https://runame.github.io/" target="_blank">37</a>, -->
  <!--  -->
  <!-- <a href="https://zyouyang.github.io/" target="_blank">38</a>, -->
  <!--  -->
  <!-- <a href="https://yohaoyu.github.io/" target="_blank">39</a>, -->
  <!--  -->
  <!-- <a href="https://jhmlam.github.io/" target="_blank">40</a>, -->
  <!--  -->
  <!-- <a href="https://lucy3.github.io/" target="_blank">41</a>, -->
  <!--  -->
  <!-- <a href="https://i-kiran.github.io/" target="_blank">42</a>, -->
  <!--  -->
  <!-- <a href="https://jasonpoulos.org/" target="_blank">43</a>, -->
  <!--  -->
  <!-- <a href="https://sanchit-ahuja.github.io/" target="_blank">44</a> -->
  <!--  -->
  <!-- ]. -->
  <!--<br/>-->
  <!-- You can find all the code needed to build your website in this <a href="https://github.com/msaveski/www_personal" target="_blank">GitHub repo</a>. -->
  <!-- Feel free to use it. <br/> -->

  <!-- If you end up using it, please link to here and drop me an email. -->
  <!-- I'm always happy to see people using it. -->
<!-- </div> -->  


    <div class="footer">
      <div class="row">
        <div class="four columns">
          Jiwan Chung
        </div>
        <div class="four columns">
          based on <a href="https://web.media.mit.edu/~msaveski/">Martin Saveski</a>'s template
        </div>
        <!-- <div class="four columns"> -->
          <!-- jiwan.chung.research@gmail.com -->
        <!-- </div> -->
        <div class="four columns">
          <!-- <span onclick="window.open('')" style="cursor: pointer"> -->
            <!-- <i class="fa-brands fa-bluesky fa-sm" style="padding-top: 10px"></i> -->
          <!-- </span> -->

          <span onclick="window.open('https://twitter.com/JiwanChung')" style="cursor: pointer">
            <i class="fa-brands fa-twitter"></i>
          </span>

          <span onclick="window.open('https://www.linkedin.com/in/jiwan-chung-81231b245')" style="cursor: pointer">
            <i class="fa-brands fa-linkedin"></i>
          </span>

          <span onclick="window.open('https://github.com/JiwanChung')" style="cursor: pointer">
            <i class="fa-brands fa-github"></i>
          </span>

          <span onclick="window.open('https://scholar.google.com/citations?user=l4UBOZAAAAAJ&hl=en')" style="cursor: pointer">
            <i class="ai ai-google-scholar" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script> -->

  <!-- do not remove -->
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
