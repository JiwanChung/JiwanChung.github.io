
<!DOCTYPE html>
<html>
<head>
	<title>Jiwan Chung</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="author" content="Jiwan Chung">
	<meta name="description" content="I do research on the social reasoning capabilities (e.g., theory of mind) of large language models.
">
	<meta property="og:image" content="/images/pic.jpg"/>
	<meta http-equiv="content-language" content="ko" />
	<meta name="generator" content="Jekyll v4.2.0">
	<meta name="naver-site-verification" content="7eb7c4883748db6c7e22ddfdc713e23042645b13"/>

	<!--[if lte IE 8]><script src="/js/ie/html5shiv.js"></script><![endif]-->
	<script src="https://kit.fontawesome.com/804808ea0a.js" crossorigin="anonymous"></script>
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css">
	<link rel="stylesheet" href="/css/main.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<!--[if lte IE 8]><link rel="stylesheet" href="/css/ie8.css" /><![endif]-->

	<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">

	<!-- <link rel="shortcut icon" href="/images/favicon.ico">
	<link rel="icon" href="/images/favicon.ico"> -->

	<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Jiwan Chung | I do research on introducing multimodality to artificial reasoners.</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Jiwan Chung" />
<meta name="author" content="Jiwan Chung" />
<meta property="og:locale" content="ko" />
<meta name="description" content="I do research on introducing multimodality to artificial reasoners." />
<meta property="og:description" content="I do research on introducing multimodality to artificial reasoners." />
<link rel="canonical" href="https://jiwanchung.github.io/" />
<meta property="og:url" content="https://jiwanchung.github.io/" />
<meta property="og:site_name" content="Jiwan Chung" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Jiwan Chung" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Jiwan Chung"},"url":"https://jiwanchung.github.io/","headline":"Jiwan Chung","name":"Strata Reloaded Template","sameAs":["https://github.com/comfusion/strata-reloaded-jekyll-template"],"description":"I do research on introducing multimodality to artificial reasoners.","@type":"WebSite","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	<link type="application/atom+xml" rel="alternate" href="https://jiwanchung.github.io/feed.xml" title="Jiwan Chung" />

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108705948-1"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-108705948-1');
	</script>


</head>

<body id="top">
	<header id="header">
		<a href="https://jiwanchung.github.io" class="image avatar"><img class="image avatar" src="/images/pic.jpg" alt="Jiwan Chung"></a>
		<h1 class="editable"><strong>Jiwan Chung</strong></h1>
		<span class="audio"></span>
		<script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
		<p>
			jiwan.chung.research@gmail.com
			<br><a href="/papers/cv.pdf" target="_blank">CV</a> <!-- | <a href="/bio/bio.txt" target="_blank">Bio</a> -->
		</p>

		<ul class="icons">
			<li>
				<a class='icon' href="https://scholar.google.co.kr/citations?user=l4UBOZAAAAAJ" target="_blank">
					<i class="ai ai-google-scholar-square ai-1x" style="font-size: 1.5em; color:#4285f4"></i>
				</a>
			</li>
			<li>
				<a class='icon' href="https://www.semanticscholar.org/author/Jiwan-Chung/2004821977" target="_blank">
					<i class="ai ai-semantic-scholar-square ai-1x" style="font-size: 1.5em; color:#0E3572"></i>
				</a>
			</li>
			
			<li>
				<a target="_blank" style="color:#000000" href="https://github.com/JiwanChung" class="icon fa-brands fa-github-square">
					<span class="label">github</span>
				</a>
			</li>
			
			<li>
				<a target="_blank" style="color:#000000" href="https://x.com/JiwanChung" class="icon fa-brands fa-square-x-twitter">
					<span class="label">x</span>
				</a>
			</li>
			
			<li>
				<a target="_blank" style="color:#0077b5" href="https://www.linkedin.com/in/jiwan-chung-81231b245/" class="icon fa-brands fa-linkedin">
					<span class="label">LinkedIn</span>
				</a>
			</li>
			
</ul>
</header>

<div id="main">
  <!-- One -->
  <section>
      <!-- <header> -->
        <!-- <h1 style="display:inline-block;">Hello,</h1> <h1 style="display:inline-block; font-size: 2.15em;">&nbspì•ˆë…•í•˜ì„¸ìš”</h1> -->
        <!-- <h2>I'm interested in where perception meets understanding.</h2> -->
      <!-- </header> -->
      <section>
        <p>
          I'm a Ph.D. candidate at Yonsei University CIP Lab, currently advised by Professor Seon Joo Kim.
        </p>

        <p>
          My research centers on multimodal understanding and generation, with a particular focus on Multimodal Large Language Models (MLLMs).
          I explore how images and text can be more effectively connected in AI systems, spanning both architectural innovations and evaluative methodologies.
        </p>

        <p>
          Here are some keywords about my current research interests:
        </p>

        <ul>
          <li>Architectural improvements to MLLMs (e.g., <a href="">Decoding</a>, <a href="">Fine-tuning</a>)</li>
          <li>Understanding and enhancing multimodal reasoning (e.g., <a href="">Grounded reasoning</a>, <a href="tbu">Fine-grained analysis</a>)</li>
          <li>Applications of multimodal reasoning (e.g., Computer-use agents, Embodied AI)</li>
        </ul>

        <!-- <p> -->
          <!-- I'm a PhD student at Yonsei University <a href="https://mirlab.yonsei.ac.kr/">MIR Lab</a>, advised by <a href="https://yj-yu.github.io/home/">Youngjae Yu</a>. -->
          <!-- Prior to this, I received my MS in Computer Engineering from <a href="https://en.snu.ac.kr/index.html">Seoul National University</a>, advised by <a href="https://vision.snu.ac.kr/gunhee/">Gunhee Kim</a>. -->
          <!-- During my PhD, I did research-oriented internships at Microsoft Research, LG AI Research, and Naver. -->
        <!-- </p> -->

        <!-- <p> -->
          <!-- My research goal is understanding how knowledge emerges and develops by replicating these processes in machines. Arguably, the most fundamental <a href="https://plato.stanford.edu/entries/epistemology/\#SourKnowJust">source of human knowledge</a> lies in perception, which forms the basis of how we interpret and learn from the world around us. To reflect this, my current focus is on multimodal artificial intelligence, where I explore challenges that arise from introducing multimodal inputs and outputs to large language models (LLMs). --> 
        <!-- </p> -->

        <!-- <p> -->
          <!-- Feel free to reach out via email if you have any questions, are interested in a potential collaboration, or simply would like to connect over coffee â˜•. -->
        <!-- </p> -->
        <!-- <p> -->
          <!-- Currently <b>seeking research internship</b> opportunities anytime through the end of 2025! ðŸ™Œ -->
        <!-- </p> -->
      </section>
  </section>

  <!-- Two -->
  <section id="two">
    <div>
      <h2>News</h2>
      <ul class='news'>
          <li>
            Aug 2025 - 1 paper have been accepted to <a href="">EMNLP 2025</a>.
          </li>
          <li>
            Jun 2025 - 1 paper have been accepted to <a href="">ICCV 2025</a>.
          </li>
          <li>
            May 2025 - 2 papers have been accepted to <a href="https://2025.aclweb.org/">ACL 2025</a>.
          </li>

          <li>
            Jan 2025 - 2 papers have been accepted to <a href="https://2025.naacl.org/">NAACL 2025 Findings</a>.
          </li>

          <!-- <li> -->
            <!-- Dec 2024 - 1 paper have been accepted to <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a>. -->
          <!-- </li> -->
        
          <!-- <li> -->
            <!-- Sep 2024 - 3 papers have been accepted to <a href="https://2024.emnlp.org/">EMNLP 2024</a>. -->
          <!-- </li> -->
        
      </ul>
    </div>
  </section>

  <!-- Three -->
  <section id="three">
    <div>
      <h2>Publications  <sup><sub>(* equal contribution)</sub></sup></h2>
        <ul style="list-style: none; padding-left: 0; margin: 0 0 0 -6px;">
          <li>
            <a class='paper' href="tbu">What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?</a><br>
            <b>Jiwan Chung</b>, Neel Joshi, Pratyusha Sharma, Youngjae Yu, Vibhav Vineet<br>
            <i>arXiv</i> 2025<br>
            *Work done as part of an internship in Microsoft<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2503.02379">[ paper ]</a> -->
          </li> <br>
          <li>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2505.18842">v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning</a><br>
            <b>Jiwan Chung*</b>, Junhyeok Kim*, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu<br>
            <i>arXiv</i> 2025<br>
            <a class="project-link" href="https://github.com/jun297/v1">github |</a>
            <a class="project-link" href="https://huggingface.co/kjunh/v1-7B">model |</a>
            <a class="project-link" href="https://huggingface.co/datasets/kjunh/v1g-sample">dataset</a>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2503.02379">Teaching Metric Distance to Discrete Autoregressive Language Models</a><br>
            <b>Jiwan Chung</b>, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu<br>
            <i>arXiv</i> 2025<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2503.02379">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2503.14427">VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms</a><br>
            Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, <b>Jiwan Chung</b>, Youngjae Yu<br>
            <i>EMNLP</i> 2025<br>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2411.14137">VAGUE: Visual Contexts Clarify Ambiguous Expressions</a><br>
            Heejeong Nam, Jinwoo Ahn, Keummin Ka, <b>Jiwan Chung</b>, Youngjae Yu<br>
            <i>ICCV</i> 2025<br>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2505.24211">Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?</a><br>
            <b>Jiwan Chung</b>, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu<br>
            <i>ACL</i> 2025<br>
            <a class="project-link" href="https://huggingface.co/datasets/jiwan-chung/ACON">dataset</a>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2506.00958">Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues</a><br>
            <b>Jiwan Chung*</b>, Youngmin Kim*, Jisoo Kim, sunghyun lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu<br>
            <i>ACL</i> 2025<br>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2410.01273">CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</a><br>
            Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, <b>Jiwan Chung</b>, Youngjae Yu<br>
            <i>ICRA</i> 2025<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2410.01273">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2502.14892">EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild</a><br>
            Junhyeok Kim, Min Soo Kim, <b>Jiwan Chung</b>, Jungbin Cho, Jisoo Kim, Sungwoong Kim, Gyeongbo Sim, Youngjae Yu<br>
            <i>NAACL Findings</i> 2025<br>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2406.14703">Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</a><br>
            Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, <b>Jiwan Chung</b>, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu<br>
            <i>NAACL Findings</i> 2025<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2406.14703">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2501.11469">MASS: Overcoming Language Bias in Image-Text Matching</a><br>
            <b>Jiwan Chung</b>, Seungwon Lim, Sangkyu Lee, Youngjae Yu<br>
            <i>AAAI</i> 2025<br>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2410.18823v1">Towards Visual Text Design Transfer Across Languages</a><br>
            <b>Jiwan Chung*</b>, Yejin Choi*, Sumin Shim, Giyeong Oh, Youngjae Yu<br>
            <i>NeurIPS Datasets and Benchmarks</i> 2024<br>
            <!-- <a class='project-link' href="tbu">[ paper ]</a> -->
            <a class="project-link" href="https://huggingface.co/datasets/yejinc/MuST-Bench">dataset</a>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2406.18925">Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding</a><br>
            <b>Jiwan Chung*</b>, Sungjae Lee*, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu<br>
            <i>EMNLP</i> 2024<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2406.18925">[ paper ]</a> -->
            <a class="project-link" href="https://github.com/JiwanChung/VisArgs">github | </a>
            <a class="project-link" href="https://huggingface.co/datasets/jiwan-chung/visargs">dataset</a>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2410.01023">Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!</a><br>
            <b>Jiwan Chung</b>, Seungwon Lim, Jaehyun Jeon, Seungbeen Lee, Youngjae Yu<br>
            <i>EMNLP</i> 2024<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2410.01023">[ paper ]</a> -->
            <a class="project-link" href="https://github.com/JiwanChung/VisualPun_UNPIE">github | </a>
            <a class="project-link" href="https://huggingface.co/datasets/jiwan-chung/VisualPun_UNPIE">dataset</a>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2404.02575">Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models</a><br>
            Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, <b>Jiwan Chung</b>, Youngjae Yu, Jinyoung Yeo<br>
            <i>EMNLP</i> 2024<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2404.02575">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2404.01954">HyperCLOVA X Technical Report</a><br>
            HyperCLOVA X Team<br>
            <i>arXiv</i> 2024<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2404.01954">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2311.01233">Long Story Short: a Summarize-then-Search Method for Long Video Question Answering</a><br>
            <b>Jiwan Chung</b>, Youngjae Yu<br>
            <i>BMVC</i> 2023<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2311.01233">[ paper ]</a> -->
            <a class="project-link" href="https://github.com/JiwanChung/long-story-short">github</a>
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2310.09767">VLIS: Unimodal Language Models Guide Multimodal Language Generation</a><br>
            <b>Jiwan Chung</b>, Youngjae Yu<br>
            <i>EMNLP</i> 2023<br>
            <a class="project-link" href="https://github.com/JiwanChung/vlis">github</a>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2310.09767">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://arxiv.org/abs/2310.10418">Reading books is great, but not if you are driving! visually grounded reasoning about defeasible commonsense norms</a><br>
            Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, <b>Jiwan Chung</b>, Yejin Son, Yejin Choi, Youngjae Yu<br>
            <i>EMNLP</i> 2023<br>
            <!-- <a class='project-link' href="https://arxiv.org/abs/2310.10418">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html">Fusing pre-trained language models with multimodal prompts through reinforcement learning</a><br>
            <b>Jiwan Chung*</b>, Youngjae Yu*, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi<br>
            <i>CVPR</i> 2023<br>
            <a class="project-link" href="https://github.com/JiwanChung/esper">github</a>
            <!-- <a class='project-link' href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://openaccess.thecvf.com/content/ICCV2021/html/Lee_ACAV100M_Automatic_Curation_of_Large-Scale_Datasets_for_Audio-Visual_Video_Representation_ICCV_2021_paper.html">ACAV100m: Automatic curation of large-scale datasets for audio-visual video representation learning</a><br>
            <b>Jiwan Chung*</b>, Sangho Lee*, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, Yale Song<br>
            <i>ICCV</i> 2021<br>
            <a class="project-link" href="https://acav100m.github.io/">webpage | </a>
            <a class="project-link" href="https://github.com/sangho-vision/acav100m">github</a>
            <!-- <a class='project-link' href="https://openaccess.thecvf.com/content/ICCV2021/html/Lee_ACAV100M_Automatic_Curation_of_Large-Scale_Datasets_for_Audio-Visual_Video_Representation_ICCV_2021_paper.html">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html">Transitional adaptation of pretrained models for visual storytelling</a><br>
            <b>Jiwan Chung*</b>, Youngjae Yu*, Heeseung Yun, Jongseok Kim, Gunhee Kim<br>
            <i>CVPR</i> 2021<br>
            <a class="project-link" href="https://github.com/JiwanChung/tapm">github</a>
            <!-- <a class='project-link' href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.html">[ paper ]</a> -->
          </li> <br>
          <li>
            <a class='paper' href="/papers/2020eccv_cisin.pdf">Character grounding and re-identification in story of videos and text descriptions</a><br>
            Youngjae Yu, Jongseok Kim, Heeseung Yun, <b>Jiwan Chung</b>, Gunhee Kim<br>
            <i>ECCV</i> 2020<br>
            <!-- <a class='project-link' href="/papers/2020eccv_cisin.pdf">[ paper ]</a> -->
          </li> <br>
        </ul>
    </div>
  </section>

  <!-- Four -->
  <section id="four">
    <div>
      <h2>Education</h2>
      <ul class='news'>
        <li>
          <a class='highlight'>Ph.D. in Artificial Intelligence, 2026 (expected) </a>, Yonsei University
        </li>
        <li>
          <i class="fa fa-graduation-cap" aria-hidden="true"></i>
          <a class='highlight'>M.S. in Computer Engineering, 2023 </a>, Seoul National University
        </li>
        <li>
          <i class="fa fa-graduation-cap" aria-hidden="true"></i>
          <a class='highlight'>B.S. in Computer Science, 2019 </a>, Yonsei University
        </li>
      </ul>
    </div>
  </section>

  <section id="four">
    <div>
      <h2>Experience</h2>
      <ul class='news'>
        <li>
          <a class='highlight'>Summer 2025</a>, Microsoft Research - AI Frontiers (Topic: Multimodal Reasoning)
        </li>
        <li>
          <a class='highlight'>Summer 2024</a>, LG AI Research - AML (Topic: Multimodal LLMs)
        </li>
        <li>
          <a class='highlight'>Summer 2023</a>, Naver - Foundational Research (Topic: Any-to-Any Foundational Models)
        </li>
      </ul>
    </div>
  </section>

  <!-- Five -->
  <!-- <section id="five"> -->
      <!-- <h2>About Me</h2> -->
      <!-- <p> -->
      <!-- </p> -->
   
  <!-- </section> -->
</div>

<!-- <footer id="footer"> -->
	<!-- <ul class="copyright"> -->
		<!-- <li>&copy; Jiwan Chung</li> -->
		<!-- <li>Design: <a href="http://html5up.net">HTML5 UP</a></li> -->
		<!-- <li>Photo credit: Sebastin Santy</li> -->
	<!-- </ul> -->
<!-- </footer> -->

<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.poptrox.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]><script src="/js/ie/respond.min.js"></script><![endif]-->
<script src="/js/main.js"></script>


</body>
</html>

